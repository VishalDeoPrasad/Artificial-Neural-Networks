{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different layers we have: \n",
    "> Layer is basically transformation of the incomming data\n",
    "1. __Core Layers__\n",
    "    1. input layer\n",
    "    1. Dense Layer\n",
    "    1. Activation Layer\n",
    "    1. Embedding Layer\n",
    "    1. Masking Layer\n",
    "    1. Lambda Layer\n",
    "1. __Convolution Layers__\n",
    "1. __Pooling Layers__   : used in images\n",
    "1. __Recurrent Layers__ : used in nlp\n",
    "1. __Preprocessing  Layers__\n",
    "1. __Normalization  Layers__\n",
    "1. __Regularization  Layers__\n",
    "1. __Attention  Layers__ : Transformer model\n",
    "1. __Reshaping  Layers__\n",
    "1. __Activation  Layers__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.layers.Dense(\n",
    "    units,\n",
    "    activation=None,\n",
    "    use_bias=True,\n",
    "    kernel_initializer=\"glorot_uniform\",\n",
    "    bias_initializer=\"zeros\",\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of sequential class; this is like a list now we need to add our layer, \n",
    "model = Sequential()\n",
    "\n",
    "# we added our hidden layer, the name of the hidden layer that keras identify is dense\n",
    "# requirement of first Dense Layer, no of neuron, activation\n",
    "# x.shape[1]- gives not of features we have\n",
    "# relu and tanh both we can use but when to use what we don't know, it is hyperparamerter tunning\n",
    "\n",
    "#input layer\n",
    "model.add(Dense(units=10, activation='relu', input_shape=(x.shape[1],)))\n",
    "#hidden layer\n",
    "model.add(Dense(units=5, activation='tanh')) #no need to give input shape; since it is sequential it know input is comming from previous layer\n",
    "#output layer\n",
    "model.add(Dense(units=1, activation='linear')) #units is depends upon type of problem we are solving, here i am solving regression problem so unit will be 1\n",
    "#activation is linear; it is also know as pass-through; it return input unmodified\n",
    "#by default is activation='linear'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = keras.optimizers.SGD(\n",
    "    learning_rate=0.01,\n",
    "    momentum=0.0,\n",
    "    nesterov=False,\n",
    "    weight_decay=None,\n",
    "    clipnorm=None,\n",
    "    clipvalue=None,\n",
    "    global_clipnorm=None,\n",
    "    use_ema=False,\n",
    "    ema_momentum=0.99,\n",
    "    ema_overwrite_frequency=None,\n",
    "    name=\"SGD\",\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the compilation code we have to make 3 important aspects;\n",
    "#loss, optimzer(gradient descent), metrics\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['r2', 'mae'])\n",
    "#We can use loss as a metric but visa versa is not possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.fit(\n",
    "    x=None,\n",
    "    y=None,\n",
    "    batch_size=None,\n",
    "    epochs=1,\n",
    "    verbose=\"auto\",\n",
    "    callbacks=None,\n",
    "    validation_split=0.0,\n",
    "    validation_data=None,\n",
    "    shuffle=True,\n",
    "    class_weight=None, #modify the loss function to pay more attention to minor class\n",
    "    sample_weight=None,\n",
    "    initial_epoch=0,\n",
    "    steps_per_epoch=None,\n",
    "    validation_steps=None,\n",
    "    validation_batch_size=None,\n",
    "    validation_freq=1,\n",
    ")\n",
    "#class_weight; How will you takle the imbalance data, not allow to over or under sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,y, epochs=100, validation_data=0.2)\n",
    "#after every iteration i will use the weight to calculate and predict the value, and use the validation to check and verify how closer or far we are\n",
    "#after every epochs, you will be able to monitor the training loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define VAE Architecture\n",
    "latent_dim = 64\n",
    "#Encoder\n",
    "encoder_input = Input(shape=(512, 512, 1))\n",
    "x = Flatten()(encoder_input)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "z_mean = Dense(latent_dim)(x)\n",
    "z_log_var = Dense(latent_dim)(x)\n",
    "\n",
    "#Sampling from the latent space\n",
    "\n",
    "def sampling(args):\n",
    "    __[1]__,__[2]__= args\n",
    "    Epsilon= tf.keras.backend.random_normal(shape=(tf.keras.backend.shape(__[1]__)[0], latent dim))\n",
    "\n",
    "    return __[1]__ + tf.keras.backend.exp(0.5* __[2]__) * epsilon\n",
    "\n",
    "z = tf.keras.layers. Lambda (sampling) ([__[1]__, __[2]__])\n",
    "\n",
    "#Decoder\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "x = Dense(256, activation ='relu')(decoder_input)\n",
    "x = Dense(512 * 512, activation='sigmoid') (x)\n",
    "decoder_output = Reshape((512, 512, 1))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VAE Architecture\n",
    "latent_dim = 64\n",
    "\n",
    "# Encoder\n",
    "encoder_input = Input(shape=(512, 512, 1))\n",
    "x = Flatten()(encoder_input)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "z_mean = Dense(latent_dim)(x)\n",
    "z_log_var = Dense(latent_dim)(x)\n",
    "\n",
    "# Sampling from the latent space\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(tf.keras.backend.shape(z_mean)[0], latent_dim))\n",
    "    return z_mean + tf.keras.backend.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "# Decoder\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "x = Dense(256, activation='relu')(decoder_input)\n",
    "x = Dense(512 * 512, activation='sigmoid')(x)\n",
    "decoder_output = Reshape((512, 512, 1))(x)\n",
    "\n",
    "# Create VAE model\n",
    "vae = Model(encoder_input, decoder_output)\n",
    "\n",
    "# Compile VAE model\n",
    "vae.compile(optimizer='adam', loss='binary_crossentropy')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
